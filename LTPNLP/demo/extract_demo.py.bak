# -*- coding: utf-8 -*-
"""
Created on Mon Jan  6 13:33:34 2020

@author: thinkpad
"""

import os
import re
import sys
sys.path.append("..") #先跳出当前目录
from core.nlp import NLP
from core.extractor import Extractor

if __name__=='__main__':
    input_path = 'D:\python-file\knowledge_extraction-master\data\\颐和园.txt' #输入的文本文件
    output_path = '../../data/knowledge_triple_颐和园_pos.json' #输入的处理结果JSON文件
    if os.path.isfile(output_path):
        os.remove(output_path)
    open(output_path,'a+')
    
    print('Strat extracting...')
    
    #实例化NLP（分词，词性标注，命名实体识别，依存句法分析）
    nlp = NLP()
    num =1 #知识三元组
    
    f_in = open(input_path,'r',encoding = 'utf-8')
    #分句，获得句子列表
    origin_sentences = re.split('[。？！; =]|\n',f_in.read())
    #遍历每一篇文档中的句子
    for origin_sentence in origin_sentences:
        origin_sentence=origin_sentence.strip()
        #原始句子长度小于6，跳过
        if(len(origin_sentence)<6):
            continue
        print('****')
        print(origin_sentence)
        #分词处理
        lemmas = nlp.segment(origin_sentence)
        #词性标注
        words_postag = nlp.postag(lemmas)
        #命名实体识别
        words_netag = nlp.netag(words_postag)
        #依存句法分析
        sentence = nlp.parse(words_netag)
        print(sentence.to_string())
        
        extractor = Extractor()
        num = extractor.extract(origin_sentence,sentence,output_path,num,nlp)
    nlp.close()