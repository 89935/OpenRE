**2020/1/15工作**
* all_entity_classification.json 按照xlore的分类对已有实体进行分类。因为同一实体
在百度百科里有歧义，尽管是匹配时已经规定了，括号前的单词必须完全匹配，但还是存在
很多相同的单词。如：硬匹配“水仙”这个实体，xlore文件中存在：
~~~~
水仙（石蒜科多年生草本植物）		植物::;观赏型植物
水仙（清代李渔的文言文）		生物物种
水仙（罗文演唱歌曲）		单曲::;娱乐作品::;音乐作品
水仙（白嘉莉演唱歌曲）		单曲::;娱乐作品::;音乐作品
水仙（电视剧《火王之破晓之战》插曲）		
水仙（民谣音乐人江生的一首单曲）		
水仙（南宋赵蕃诗作）		
.............................
~~~~
我们用`||`来将从不同的实体得到的分类结果分开，最后得出结果：
~~~~
"水仙": "植物;观赏型植物||生物物种||单曲;娱乐作品;音乐作品||单曲;娱乐作品;音乐作品||生活;生活术语||单曲;娱乐作品;流行音乐;音乐作品||单曲;娱乐作品;音乐作品||单曲;娱乐作品;音乐作品||音乐||||单曲;娱乐作品;流行音乐;音乐作品||植物;生物物种||中国文学;书籍;出版物;娱乐作品;小说;小说作品;文学作品;艺术书籍||单曲;娱乐作品;流行音乐;音乐作品||书籍;文学作品;自然||音乐||单曲;音乐;音乐作品||娱乐;音乐||植物;生物物种;自然||戏剧;文学作品||戏剧;文学作品||文化||||||||||||||||||||||||||||||||||||||||",
~~~~
* all_entity_context.json:将命中实体的上下文取出来
~~~~
"同治": ["房，年幼登基的康熙、同治、光绪三位皇帝均在此", "院黄地蓝寿字纹碗=清同治，高7.5cm，口径", "。足内施白釉书红彩“同治年制”四字楷书款。碗", "上下由疏至密。该碗是同治时期御窑厂为慈禧太后", "蝠金团寿字纹盖碗=清同治，通高通高8.8cm", "足内施白釉，书红彩“同治年制”4字楷书款。盖", "装饰排列疏密有致，是同治时期御窑厂为慈禧太后", "彩五福捧寿纹奓斗=清同治，高8.9cm，口径", "一周。底白釉红彩书“同治年制”4字楷书款。此", "（福）捧寿”。该器是同治时期御窑厂为慈禧太后", "彩团“寿”字渣斗=清同治，高8.6cm，口径", "釉。外底署红彩楷书“同治年制”双行四字款。渣", "=黄地蓝寿字纹盘，清同治，高5.1cm，口径", "。足内施白釉书红彩“同治年制”4字楷书款。此", "”4字楷书款。此盘是同治时期御窑厂为慈禧太后", "为大学士和珅的私邸。同治初年，这里的第三代主", "以重任的人。因此，从同治朝起，奕劻便得到慈禧", "处私宅便赠给了庆王。同治朝时，由于恭亲王奕忻", "锦吟》卷七中有“嗣于同治年间邸园落成”之记；", "之记；而“样子雷”于同治四年绘制的多幅恭王府", "，所有这些都印证着：同治年间的那次“修整”，", "的祖父厉子嘉，在清朝同治和光绪年间，任内务府", "遗址上督建而成。（《同治十二年迁安县志》记载", "史=给你好看的历史=同治重修圆明园遭到抵制慈"],
~~~~
* context_word_freq_dict.json：将命中实体的上下文进行分词，再进行词性标注，将词语和词性标注结合，再统计出现的次数，根据词频进行排序。
~~~~
"中和殿": [
		["中和殿_n", 6],
		["保和殿_ns", 4],
		["太和殿_ns", 3],
		["中和殿_ns", 3],
		["博物院_n", 2],
		["明_nt", 2],
		["永乐十八年_nt", 2],
		["公元_n", 2],
		["位于_v", 2],
		["以北_nd", 2],
		["筑=_v", 1],
		["养_v", 1],
		["南半部_ni", 1],
		["三大殿_nz", 1],
		["中心_n", 1],
		["北京故宫_ns", 1],
		...........
	]
~~~~
这样问题在于，因为在词性标注时，可能将同一个实体用不同的词性标注，见上`["中和殿_n", 6],["中和殿_ns", 3]`
* context_only_word_freq_dict.json:针对上述问题，我们只进行分词，不进行词性标注。
同样对于实体“中和殿”，我们得到：`["中和殿", 9]`,

~~~~
"中和殿": [
    ["中和殿", 9],
    ["保和殿", 4],
    ["太和殿", 3],
    ["博物院", 2],
    ["明", 2],
    ["永乐十八年", 2],
    ["公元", 2],
    ["位于", 2],
    ["以北", 2],
    ["筑=", 1],
    ["养", 1],
    ["南半部", 1],
    ...........
]
~~~~
* 注：分词用的是：TLP，将之前识别出来的实体all_entity作为用户自定义词汇，加入LTP模型中，
这样就不怕分词将一个实体给分乱了。
对于恭王府文档中出现的一个句子：
`2005年1月北京市决定将地处朝阳区东四环原准备到土地市场上进行交易的学校，改造为附中，2006年完成搬迁`

LTP不加入自定义词汇的结果：
~~~~
2005年	1月	北京市	决定	将	地处	朝阳区	东四	环	原	准备	到	土地	市场	上	进行	交易	的	学校	,	改造	为	附中	,	2006年	完成	搬迁
nt	nt	ns	v	p	v	ns	ns	n	d	v	v	n	n	nd	v	v	u	n	wp	v	v	j	wp	nt	v	v
~~~~
LTP加入自定义词汇的结果：因为自定义词汇中加入了“东四环”和“土地市场”，所以这两个实体没有被分开。
~~~~
2005年	1月	北京市	决定	将	地处	朝阳区	东四环	原	准备	到	土地市场	上	进行	交易	的	学校	,	改造	为	附中	,	2006年	完成	搬迁
nt	nt	ns	v	p	v	ns	ns	d	v	v	n	nd	v	v	u	n	wp	v	v	j	wp	nt	v	v
~~~~
同样地，jieba分词也能做到这一点。